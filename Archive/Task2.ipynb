{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e2qE1c-XzdyD"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import re\n",
        "from random import sample, shuffle\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skayYlFelBmG",
        "outputId": "e82c7cc2-c83c-4411-b8f3-96d81d007d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNui7uQP0FXc",
        "outputId": "29d52a1d-e999-4432-e87b-7a27cbf3cf7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# I used google collab for training my model, uncomment the lines below to \n",
        "# connect to google drive in google colab\n",
        "# NB corpus_root SHOULD BE CHANGED TO MATC THE CORPUS PATH ON THE SPECIFIC MACHINE\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9M8MrkbzdyE",
        "outputId": "0d9d3450-5335-41ac-9143-2f40d3e38ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
          ]
        }
      ],
      "source": [
        "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords = False)\n",
        "# Folder path where corpus root should be\n",
        "corpus_root = r\"/content/drive/MyDrive/cw2/product_reviews\"\n",
        "file_pattern = r\".*\"\n",
        "original_corpus = nltk.corpus.PlaintextCorpusReader(corpus_root, file_pattern)\n",
        "print(original_corpus.fileids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3DsjTya9zdyF"
      },
      "outputs": [],
      "source": [
        "# Core utility function for document cleaning\n",
        "# Works recursively, split the text into sentences/review, then for each sentence/review perform cleaning \n",
        "def process_doc(text, remove_punctuation, case_fold, stem,\n",
        "                remove_stopwords, remove_short_tokens, tokenize_by, stem_blacklist = [],\n",
        "                remove_nonalphabetical = False):\n",
        "\n",
        "  if (tokenize_by == \"sentence\"):\n",
        "    sentences = nltk.RegexpTokenizer(\"##\", gaps = True).tokenize(text)\n",
        "    sentences = [process_doc(sentence, remove_punctuation, case_fold, stem, \n",
        "                             remove_stopwords, remove_short_tokens, \"words\", stem_blacklist) \n",
        "                  for sentence in sentences]\n",
        "    return sentences\n",
        "  if (tokenize_by == \"reviews\"):\n",
        "    reviews = nltk.RegexpTokenizer(\"\\[ t \\]\", gaps = True).tokenize(text)\n",
        "    reviews = [process_doc(review, remove_punctuation, case_fold, stem, \n",
        "                              remove_stopwords, remove_short_tokens, \"words\", stem_blacklist)\n",
        "                for review in reviews]\n",
        "    return reviews\n",
        "  if (tokenize_by == \"words\"):\n",
        "    words = nltk.TreebankWordTokenizer().tokenize(text)\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "      words = [w.strip(\"\") for w in words]\n",
        "      words = [w.strip(\".\") for w in words]\n",
        "    if (case_fold):\n",
        "      words = [w.lower() for w in words]\n",
        "    if (remove_short_tokens):\n",
        "      words = [w for w in words if len(w) > 2]\n",
        "    if (stem):\n",
        "      words = [w if w in stem_blacklist else stemmer.stem(w) for w in words]\n",
        "    if (remove_stopwords):\n",
        "      words = [w for w in words if w not in stop_words and w != \"n't\"]\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "    if (remove_nonalphabetical):\n",
        "      words = [w for w in words if w.isalpha()]\n",
        "    return words\n",
        "\n",
        "def process_corpus(corpus, remove_punctuation:bool, case_fold:bool, stem:bool,\n",
        "                  remove_stopwords:bool, remove_short_tokens, tokenize_by:str, remove_nonalphabetical):\n",
        "  docs = [word for fileid in corpus.fileids() \n",
        "            for word in process_doc(corpus.raw(fileid), remove_punctuation, case_fold,\n",
        "                                    stem, remove_stopwords, remove_short_tokens, \n",
        "                                    tokenize_by, remove_nonalphabetical)\n",
        "         ]\n",
        "  return docs\n",
        "\n",
        "def most_frequent(words, n, should_print):\n",
        "  freqDist = nltk.FreqDist(words)\n",
        "  most_common = freqDist.most_common(n)\n",
        "  if (should_print):\n",
        "    i = 1\n",
        "    for (w, count) in most_common:\n",
        "      print(i , w , count)\n",
        "      i += 1\n",
        "  return most_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QoqkRnCmzdyG"
      },
      "outputs": [],
      "source": [
        "def get_all_sentences_cleaned(corpus_filepath):\n",
        "  corpus = nltk.corpus.PlaintextCorpusReader(corpus_filepath, file_pattern)\n",
        "  out = []\n",
        "  for fileid in corpus.fileids():\n",
        "    sentences = process_doc(corpus.raw(fileid), True, True, True, True, True,\"sentence\", [\"not\"], False)\n",
        "    out.extend(sentences)\n",
        "  return out\n",
        "\n",
        "# partitions corpus into sentiments, and cleans the text\n",
        "def get_all_sentiments_cleaned(corpus_filepath, stemming, stop_words, remove_mixed_sentiments = False):\n",
        "  corpus = nltk.corpus.PlaintextCorpusReader(corpus_filepath, file_pattern)\n",
        "  # pattern used to match sentiments \n",
        "  pattern = re.compile(r\"(([a-z -]*\\[[\\-\\+][0-9]\\],? ?)+#[^(\\[)]+)\")\n",
        "  sentiments = []\n",
        "  for file_id in corpus.fileids():\n",
        "    text = corpus.raw(file_id)\n",
        "    text = re.sub(\"\\[[a-z]+\\]\", \"\", text)\n",
        "    text = pattern.findall(text)\n",
        "    for sentiment in text:\n",
        "      sentiment_parsed = sentiment[0]\n",
        "      # Find all labels for whether a sentiment is positive or negative\n",
        "      matches = re.findall(\"\\[[\\+\\-][0-9]\\]\", sentiment_parsed)\n",
        "      score = 0\n",
        "      has_positive = False\n",
        "      has_negative = False\n",
        "      for match in matches:\n",
        "        score += int(match[1:-1])\n",
        "        if match[1] == \"+\":\n",
        "          has_positive = True\n",
        "        if match[1] == \"-\":\n",
        "          has_negative = True\n",
        "      # if the sum of all scores is 0 discard the sample, since we are doing binary \n",
        "      # classification, optionally remove all sentiments with mixed labels\n",
        "      if remove_mixed_sentiments and has_positive and has_negative:\n",
        "        continue\n",
        "      if (score == 0): continue\n",
        "      if (score < 0): score = 0\n",
        "      if (score > 0): score = 1\n",
        "      sentiment_parsed = process_doc(sentiment_parsed, True, True, stemming, stop_words, True, \"words\", [], True)\n",
        "      if (len(sentiment_parsed[:-1]) < 2): continue\n",
        "      sentiments.append((sentiment_parsed[:-1], score))\n",
        "  return sentiments\n",
        "\n",
        "def generate_word_to_indx_and_idx_to_word(corpus):\n",
        "  word_to_idx = {}\n",
        "  idx_to_word = {}\n",
        "  i = 0\n",
        "  for sentence in corpus:\n",
        "    for word in sentence[0]:\n",
        "      if (word not in word_to_idx):\n",
        "        word_to_idx[word] = i\n",
        "        idx_to_word[i] = word\n",
        "        i += 1\n",
        "  return (word_to_idx, idx_to_word)\n",
        "\n",
        "def get_context_window_tuples(word_to_idx, sentences, window, key_words):\n",
        "  tuples = []\n",
        "  for sentence in sentences:\n",
        "    for i in range(window, len(sentence) - window):\n",
        "      # if sentence[i] in key_words:\n",
        "        context = []\n",
        "        middle_word = word_to_idx[sentence[i]]\n",
        "        for j in range (i - window, i + window + 1):\n",
        "          if i != j:\n",
        "            context.append(word_to_idx[sentence[j]])\n",
        "        tuples.append((context, word_to_idx[sentence[i]]))\n",
        "          \n",
        "        \n",
        "  return tuples\n",
        "\n",
        "\n",
        "def get_skipgrams(sentiments, window):\n",
        "  word = []\n",
        "  context = []\n",
        "  for sentiment in sentiments:\n",
        "    sentence = sentiment[0]\n",
        "    for i in range(len(sentence)):\n",
        "      cont = [sentence[idx] for idx in range(max(0, i - window), min(len(sentence), i + window + 1)) if idx != i]\n",
        "      word.extend([sentence[i]] * (len(cont)))\n",
        "      context.extend(cont)\n",
        "  return(word, context)     \n",
        "\n",
        "def get_batches(words, contexts, batch_size):\n",
        "  shuffled_idxs = sample(range(0, len(words)), len(words))\n",
        "  batches = []\n",
        "\n",
        "  batch_word, batch_context = [], []\n",
        "  for i in range(len(words)):\n",
        "    idx = shuffled_idxs[i]\n",
        "    batch_word.append(words[idx])\n",
        "    batch_context.append(contexts[idx])\n",
        "    if (i + 1) % batch_size == 0 or i + 1 == len(words):\n",
        "      batches.append((\n",
        "        torch.from_numpy(np.array(batch_word)),\n",
        "        torch.from_numpy(np.array(batch_context))\n",
        "      ))\n",
        "      batch_word, batch_context = [], []\n",
        "  return batches\n",
        "  \n",
        "def get_x_tensors(x_y_tuples):\n",
        "  tensors = []\n",
        "  for tuple in x_y_tuples:\n",
        "    tensors.append(torch.tensor(tuple[0], dtype=torch.long))\n",
        "  return tensors\n",
        "\n",
        "\n",
        "def get_y_tensors(tuples, num_classes):\n",
        "  tensors = []\n",
        "  \n",
        "  for tuple in tuples:\n",
        "    tensors.append(F.one_hot(torch.tensor(tuple[1]), num_classes=num_classes))\n",
        "  return tensors\n",
        "\n",
        "def get_sentiments_as_word_idxs(sentiments, word_to_idx):\n",
        "  return [([word_to_idx[word] for word in words], label) for (words, label) in sentiments]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3awLW7zjF1cw"
      },
      "outputs": [],
      "source": [
        "# Function to split data into K folds\n",
        "def k_fold_partititoning(sentiments, k, should_shuffle):\n",
        "  # shuffle\n",
        "  # we do not shuffle when we need to compare the results of experiments\n",
        "  if (should_shuffle):\n",
        "    shuffle(sentiments)\n",
        "  folds = []\n",
        "  # determine fold size\n",
        "  partition_step = len(sentiments) // k\n",
        "  remainders = len(sentiments) % k\n",
        "  start = 0\n",
        "  # append to each fold\n",
        "  for i in range(k):\n",
        "    if (remainders > 0):\n",
        "      folds.append(sentiments[start : start + partition_step + 1])\n",
        "      start += partition_step + 1\n",
        "      remainders -= 0\n",
        "    else:\n",
        "      folds.append(sentiments[start : start + partition_step])\n",
        "      start += partition_step\n",
        "  return folds\n",
        "\n",
        "# partition the data into two - the i-th fold and the rest\n",
        "def split_training_testing_from_k_folds(i, folds):\n",
        "  # To ensure that not tampering is done \n",
        "  testing = copy.deepcopy(folds[i])\n",
        "  \n",
        "  training = []\n",
        "  for j in range(i):\n",
        "    training.extend(folds[j])\n",
        "  for j in range(i + 1, len(folds)):\n",
        "    training.extend(folds[j])\n",
        "  return (training, testing)\n",
        "  \n",
        "def to_tensors(sentiments):\n",
        "  shuffle(sentiments)\n",
        "  start = 0\n",
        "  batches = [(torch.from_numpy(np.array(sentiment[0])), torch.from_numpy(np.array(sentiment[1], dtype=float))) for sentiment in sentiments]\n",
        "  return batches\n",
        "\n",
        "# y_hat is a tensor output of a sigmoid (y_hat between: [0, 1])\n",
        "def get_binary_accuracy(y_hat, y, verbose=False):\n",
        "  # if y_hat <= 0.5: rounded = 0 else: rounded = 1\n",
        "  rounded = torch.round(y_hat)\n",
        "  correct = (rounded == y).float()\n",
        "  if verbose:\n",
        "    print(\"y_hat:\", y_hat.data)\n",
        "    print(\"y:\", y.data)\n",
        "    print(\"rounded:\", rounded.data)\n",
        "    print(\"correct: \", correct.data)\n",
        "  return correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PhRMLmLaND0h"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, n_filters, embedding_dim = None, padding_idx = None, embedding_weights = None, dropout_rate = 0):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        if (embedding_weights != None):\n",
        "          self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False)\n",
        "          embedding_dim = embedding_weights.size()[1]\n",
        "        else:\n",
        "          self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = padding_idx)\n",
        "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (3, embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (4, embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (5, embedding_dim))\n",
        "        \n",
        "        self.fc = nn.Linear(3 * n_filters, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        \n",
        "    def forward(self, text, training = False):\n",
        "  \n",
        "        embedding = self.embedding(text)\n",
        "        #embedding = [len(text) x embedding_size] \n",
        "\n",
        "        embedding = embedding.unsqueeze(1)\n",
        "        embedding = embedding.unsqueeze(1)\n",
        "        embedding = embedding.permute(1, 2, 0, 3)\n",
        "\n",
        "        conved_0 = F.relu(self.conv_0(embedding).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedding).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedding).squeeze(3))\n",
        "        #conved_n = [len(text) - kernel_size x number of filters] \n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        concat = torch.cat((\n",
        "            pooled_0, \n",
        "            pooled_1, \n",
        "            pooled_2)\n",
        "          , dim = 1)\n",
        "        # Apply dropout only when training\n",
        "        if (training):\n",
        "          concat = self.dropout(concat)\n",
        "        \n",
        "        #concat = [len(text) - kernel_size x number of filters] \n",
        "        return self.sigmoid(self.fc(concat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b0bGxoGvkQl5"
      },
      "outputs": [],
      "source": [
        "def train_and_eval(num_filters, embedding_size, epochs, batch_size, learning_rate, folds, vocab_size, padding_idx, dropout_rate, verbose):\n",
        "  K = len(folds)\n",
        "  # accuracies will contain the accuracies for each fold for each epoch\n",
        "  accuracies = np.zeros((K, epochs))\n",
        "  for k in range(K):\n",
        "    if (verbose):\n",
        "      print(\"FOLD: \", k + 1)\n",
        "    (training_data, testing_data) = split_training_testing_from_k_folds(k, folds)\n",
        "    model = CNN(vocab_size + 1, num_filters, embedding_size, padding_idx, dropout_rate=dropout_rate)\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "    loss_fn = nn.BCELoss()\n",
        "    loss_fn = loss_fn.to(device)\n",
        "    for epoch in range(epochs):\n",
        "      if (verbose):\n",
        "        print(\"EPOCH:\", epoch + 1)\n",
        "      acc = 0\n",
        "      total_loss = 0\n",
        "      n = 0\n",
        "      for sample in to_tensors(training_data):\n",
        "        optimizer.zero_grad()\n",
        "        (sentiment, label) = sample \n",
        "        sentiment = sentiment.to(device)\n",
        "        label = label.to(device)\n",
        "        y_hat = model(sentiment, training = True).squeeze()\n",
        "        acc += get_binary_accuracy(y_hat, label, 1 == 0)\n",
        "        loss = loss_fn(y_hat, label.float())\n",
        "        total_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        n += 1\n",
        "      if (verbose):\n",
        "        print(\"TRAINING: accuracy\", (acc / n).item(), \"total loss\", total_loss.item())\n",
        "      with torch.no_grad():\n",
        "        accuracy = 0\n",
        "        for (sentiment, label) in to_tensors(testing_data):\n",
        "          sentiment = sentiment.to(device)\n",
        "          label = label.to(device)\n",
        "          accuracy += get_binary_accuracy(model(sentiment), label)\n",
        "        accuracies[k][epoch] = (accuracy / len(testing_data)).item()\n",
        "        if (verbose):\n",
        "          print(\"EPOCH VALIDATION ACCURACY\", accuracy.item())\n",
        "  \n",
        "  # epoch averages contains the average accuracy for each epoch accross all folds\n",
        "  epoch_averages = np.mean(accuracies, axis=0)\n",
        "  best_epoch = np.argmax(epoch_averages)\n",
        "  return \"Best epoch:\", best_epoch + 1, \"with an average\", epoch_averages[best_epoch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7U5SQNal3psM"
      },
      "outputs": [],
      "source": [
        "def run_experiment(num_filters, embedding_size, epochs, batch_size, learning_rate, stemming, stopword_removal, dropout, verbose, should_shuffle, remove_mixed = False):\n",
        "  # sentiments - a list of tuples, tuple[0] is the cleaned text of a sentiment, tuple[1] is {0, 1} represents if a sentiment is positive(1) or negative(0)\n",
        "  sentiments = get_all_sentiments_cleaned(corpus_root, stemming, stopword_removal, remove_mixed)\n",
        "  (word_to_idx, idx_to_word) = generate_word_to_indx_and_idx_to_word(sentiments)\n",
        "  # tuple[0] in sentiments becomes a list of ints, each int represents a token, word_to_idx, idx_to_word contain the mapping\n",
        "  sentiments = get_sentiments_as_word_idxs(sentiments, word_to_idx)\n",
        "  PADDING_STR = \"<pad>\"\n",
        "  PADDING_IDX = len(word_to_idx)\n",
        "  idx_to_word[PADDING_IDX] = PADDING_STR\n",
        "  word_to_idx[PADDING_STR] = PADDING_IDX\n",
        "  vocab_size = len(idx_to_word)\n",
        "  # The filter size of the CNN is 5, all shorter texts than that need padding\n",
        "  for sentiment in sentiments:\n",
        "    while (len(sentiment[0]) < 5): \n",
        "      sentiment[0].append(PADDING_IDX)\n",
        "  k_folds = k_fold_partititoning(sentiments, 5, should_shuffle)\n",
        "  \n",
        "  # training and evaluation\n",
        "  return train_and_eval(num_filters, embedding_size, epochs, batch_size, learning_rate, k_folds, vocab_size, PADDING_IDX, dropout, verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHEaMVNs05Pd",
        "outputId": "71d69f9d-f31f-48cd-8cd0-5d1b503ca94c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing mixed sentiments ('Best epoch:', 5, 'with an average', 0.7084891080856324)\n",
            "Keeping mixed sentiments ('Best epoch:', 5, 'with an average', 0.6722772240638732)\n"
          ]
        }
      ],
      "source": [
        "print(\"Removing mixed sentiments\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 5, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, should_shuffle = True, remove_mixed = True))\n",
        "\n",
        "print(\"Keeping mixed sentiments\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 5, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, should_shuffle = False, remove_mixed = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN0s-oY5VtaP",
        "outputId": "9fbb7232-de88-4202-c594-5120ac016db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With stemming ('Best epoch:', 13, 'with an average', 0.6985148429870606)\n",
            "With stop words removal ('Best epoch:', 9, 'with an average', 0.6851130485534668)\n",
            "Baseline ('Best epoch:', 6, 'with an average', 0.7138613820075989)\n"
          ]
        }
      ],
      "source": [
        "print(\"With stemming\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=True, stopword_removal = False, dropout = 0.75, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"With stop words removal\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = True, dropout = 0.75, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Baseline\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, should_shuffle = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyb9b5RvczIu",
        "outputId": "563ea15a-7942-41a5-cf48-31f373d09128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropout 0 ('Best epoch:', 20, 'with an average', 0.6866336584091186)\n",
            "Dropout 0.25 ('Best epoch:', 16, 'with an average', 0.6985148549079895)\n",
            "Dropout 0.50 ('Best epoch:', 9, 'with an average', 0.7054455399513244)\n",
            "Dropout 0.75 ('Best epoch:', 6, 'with an average', 0.705940580368042)\n",
            "Dropout 0.85 ('Best epoch:', 6, 'with an average', 0.7113861203193664)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dropout 0\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Dropout 0.25\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.25, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Dropout 0.50\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.50, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Dropout 0.75\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.75, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Dropout 0.85\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ig1kZ70vWCDN",
        "outputId": "8fb23d7a-570c-4e24-d075-9d1f65e57249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropout 0.95 ('Best epoch:', 18, 'with an average', 0.6539603888988494)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dropout 0.95\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.95, verbose = False, should_shuffle = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYIN4rVXdMGl",
        "outputId": "4379ce30-e78b-4817-f9f1-3c453b4ad2e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filter number 50: ('Best epoch:', 9, 'with an average', 0.6836633563041687)\n",
            "Filter number 100 ('Best epoch:', 6, 'with an average', 0.6985148429870606)\n",
            "Filter number 200 ('Best epoch:', 8, 'with an average', 0.6693069219589234)\n",
            "Filter number 300 ('Best epoch:', 9, 'with an average', 0.6623762249946594)\n",
            "Filter number 400 ('Best epoch:', 9, 'with an average', 0.6757425785064697)\n"
          ]
        }
      ],
      "source": [
        "# Running for only 15 epochs to speed up the experiment \n",
        "print(\"Filter number 50:\", run_experiment(num_filters = 50, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Filter number 100\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Filter number 200\", run_experiment(num_filters = 200, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Filter number 300\", run_experiment(num_filters = 300, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Filter number 400\", run_experiment(num_filters = 400, embedding_size = 300, epochs = 15, batch_size = 150, \n",
        "                                      learning_rate=0.001, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxVb-7C62ZcN",
        "outputId": "67b0022f-5868-4407-e667-98b79a6c029e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding size 50 ('Best epoch:', 11, 'with an average', 0.6975247383117675)\n",
            "Embedding size 100 ('Best epoch:', 15, 'with an average', 0.6891089081764221)\n",
            "Embedding size 200 ('Best epoch:', 10, 'with an average', 0.7064356327056884)\n",
            "Embedding size 300 ('Best epoch:', 13, 'with an average', 0.7143564343452453)\n",
            "Embedding size 400 ('Best epoch:', 12, 'with an average', 0.7099009871482849)\n"
          ]
        }
      ],
      "source": [
        "print(\"Embedding size 50\", run_experiment(num_filters = 100, embedding_size = 50, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Embedding size 100\", run_experiment(num_filters = 100, embedding_size = 100, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "\n",
        "print(\"Embedding size 200\", run_experiment(num_filters = 100, embedding_size = 200, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        " \n",
        "print(\"Embedding size 300\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))\n",
        "  \n",
        "print(\"Embedding size 400\", run_experiment(num_filters = 100, embedding_size = 400, epochs = 20, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPpx0Ty9nHGC",
        "outputId": "ab7598c6-b4d8-45fd-d207-5a50b19ac953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with best parameters: ('Best epoch:', 28, 'with an average', 0.7410756111145019)\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy with best parameters:\", run_experiment(num_filters = 100, embedding_size = 300, epochs = 30, batch_size = 150, \n",
        "                                      learning_rate=0.0005, stemming=False, stopword_removal = False, dropout = 0.85, verbose = False, should_shuffle = True, remove_mixed = True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Task2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7c921179475be10ab3872e10c2f4280fa2b556ee0d581886f6fc4b5fe61deff4"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
