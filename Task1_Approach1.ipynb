{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords = False)\n",
    "# Folder path where corpus root should be\n",
    "corpus_root = r\"/Users/boykoborisov/Desktop/Uni/COMP34711/cw2/product_reviews\"\n",
    "# Folder path where the reverse token corpus should be stored\n",
    "corpus_after_token_reversal = r\"/Users/boykoborisov/Desktop/Uni/COMP34711/cw2/product_reviews_processed\"\n",
    "file_pattern = r\".*\"\n",
    "original_corpus = nltk.corpus.PlaintextCorpusReader(corpus_root, file_pattern)\n",
    "print(original_corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core utility function for document cleaning\n",
    "# Works recursively, split the text into sentences/review, then for each \n",
    "# sentence/review perform cleaning \n",
    "def process_doc(text, remove_punctuation, case_fold, stem,\n",
    "                remove_stopwords, remove_short_tokens, tokenize_by, stem_blacklist = [],\n",
    "                remove_nonalphabetical = False):\n",
    "\n",
    "  if (tokenize_by == \"sentence\"):\n",
    "    sentences = nltk.RegexpTokenizer(\"##\", gaps = True).tokenize(text)\n",
    "    sentences = [process_doc(sentence, remove_punctuation, case_fold, stem, \n",
    "                             remove_stopwords, remove_short_tokens, \"words\", stem_blacklist) \n",
    "                  for sentence in sentences]\n",
    "    return sentences\n",
    "  if (tokenize_by == \"reviews\"):\n",
    "    reviews = nltk.RegexpTokenizer(\"\\[ t \\]\", gaps = True).tokenize(text)\n",
    "    reviews = [process_doc(review, remove_punctuation, case_fold, stem, \n",
    "                              remove_stopwords, remove_short_tokens, \"words\", stem_blacklist)\n",
    "                for review in reviews]\n",
    "    return reviews\n",
    "  if (tokenize_by == \"words\"):\n",
    "    words = nltk.WordPunctTokenizer().tokenize(text)\n",
    "    if (remove_punctuation):\n",
    "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
    "      # words = [w.strip(\"\") for w in words]\n",
    "    if (case_fold):\n",
    "      words = [w.lower() for w in words]\n",
    "    if (remove_short_tokens):\n",
    "      words = [w for w in words if len(w) > 2]\n",
    "    if (stem):\n",
    "      words = [w if w in stem_blacklist else stemmer.stem(w) for w in words]\n",
    "    if (remove_stopwords):\n",
    "      words = [w for w in words if w not in stop_words and w != \"n't\"]\n",
    "    if (remove_punctuation):\n",
    "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
    "    if (remove_nonalphabetical):\n",
    "      words = [w for w in words if w.isalpha()]\n",
    "    return words\n",
    "\n",
    "def process_corpus(corpus, remove_punctuation:bool, case_fold:bool, stem:bool,\n",
    "                  remove_stopwords:bool, remove_short_tokens, tokenize_by:str):\n",
    "  docs = [word for fileid in corpus.fileids() \n",
    "            for word in process_doc(corpus.raw(fileid), remove_punctuation, case_fold,\n",
    "                                    stem, remove_stopwords, remove_short_tokens, \n",
    "                                    tokenize_by)\n",
    "         ]\n",
    "  return docs\n",
    "\n",
    "def most_frequent(words, n, should_print):\n",
    "  freqDist = nltk.FreqDist(words)\n",
    "  most_common = freqDist.most_common(n)\n",
    "  if (should_print):\n",
    "    i = 1\n",
    "    for (w, count) in most_common:\n",
    "      print(i , w , count)\n",
    "      i += 1\n",
    "  return most_common\n",
    "\n",
    "# core function for generating corpus with reversed words\n",
    "# the corpus of reversed words is stored as files in the path specified by the variable:\n",
    "# corpus_after_token_reversal\n",
    "def generate_corpus_half_tokens_reversed(corpus, token_tuple_list, override_folder):\n",
    "  if not override_folder and os.path.exists(corpus_after_token_reversal):\n",
    "    return\n",
    "  if not os.path.exists(corpus_after_token_reversal):\n",
    "    os.mkdir(corpus_after_token_reversal)\n",
    "  # indecies_per_word = {word : list of 0s and 1s}\n",
    "  # if indecies_per_word[word][i] == 1\n",
    "  # the i-th occurrence of word needs to be reversed\n",
    "  indecies_per_word = {}\n",
    "  \n",
    "  # pointers keeps track of how many occurrences of each word we have met\n",
    "  pointers = {}\n",
    "  for (word, frequency) in token_tuple_list:\n",
    "    # construct an array with an equal number of 0-s and ones\n",
    "    indecies = np.ones(frequency)\n",
    "    indecies[:int(frequency/2)] = 0\n",
    "    \n",
    "    # shuffle it    \n",
    "    np.random.shuffle(indecies)\n",
    "    indecies_per_word[word] = indecies\n",
    "    pointers[word] = -1\n",
    "  fileids = corpus.fileids()\n",
    "  for fileid in fileids:\n",
    "    # tokenize the document\n",
    "    tokens = process_doc(corpus.raw(fileid), False, True, False, False, False, \"words\")\n",
    "    with_reversal = []\n",
    "    for token in tokens:\n",
    "      if (token in indecies_per_word):\n",
    "        # update the number of occurrences of the token\n",
    "        pointers[token] += 1\n",
    "       # determine whether to reverse the token\n",
    "        if (indecies_per_word[token][pointers[token]] == 1):\n",
    "         token = token[::-1]\n",
    "      with_reversal.append(token)\n",
    "    doc = \" \".join(with_reversal)\n",
    "    \n",
    "    f = open(os.path.join(corpus_after_token_reversal,fileid), \"w\")\n",
    "    f.write(doc)\n",
    "    f.close()\n",
    "  \n",
    "  # for (word, pointer) in pointers.items():\n",
    "  #   print (word, len(indecies_per_word[word]) - pointer - 1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent 50 tokens in corpus after document cleaning\n",
      "1 use 353\n",
      "2 phone 320\n",
      "3 one 316\n",
      "4 ipod 314\n",
      "5 router 313\n",
      "6 camera 292\n",
      "7 player 269\n",
      "8 get 252\n",
      "9 battery 239\n",
      "10 like 195\n",
      "11 great 192\n",
      "12 quality 176\n",
      "13 good 176\n",
      "14 zen 174\n",
      "15 diaper 171\n",
      "16 product 166\n",
      "17 would 158\n",
      "18 also 156\n",
      "19 time 145\n",
      "20 software 145\n",
      "21 sound 144\n",
      "22 well 138\n",
      "23 really 136\n",
      "24 micro 136\n",
      "25 features 128\n",
      "26 computer 128\n",
      "27 easy 125\n",
      "28 even 123\n",
      "29 first 121\n",
      "30 used 120\n",
      "31 creative 118\n",
      "32 much 115\n",
      "33 better 114\n",
      "34 champ 113\n",
      "35 work 112\n",
      "36 want 107\n",
      "37 size 105\n",
      "38 music 105\n",
      "39 norton 104\n",
      "40 little 101\n",
      "41 need 100\n",
      "42 pictures 99\n",
      "43 works 99\n",
      "44 still 97\n",
      "45 buy 96\n",
      "46 problem 96\n",
      "47 mp3 96\n",
      "48 price 91\n",
      "49 life 91\n",
      "50 using 91\n"
     ]
    }
   ],
   "source": [
    "print(\"Most frequent 50 tokens in corpus after document cleaning\")\n",
    "processed_corpus = process_corpus(original_corpus, True, True, False, True, True, \"words\")\n",
    "most_frequent_tokens = most_frequent(processed_corpus, 50, True)\n",
    "cluster_words = set()\n",
    "for (word, freq) in most_frequent_tokens:\n",
    "  cluster_words.add(word)\n",
    "  cluster_words.add(word[::-1])\n",
    "# generate_corpus_half_tokens_reversed(original_corpus, most_frequent_tokens, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a dictionary, where for each word we have its number of coappearences with other words\n",
    "def generate_term_to_term_dict_single_doc(term_groups, context_window_length, key_words):\n",
    "  term_to_term = defaultdict(lambda: defaultdict(int))\n",
    "  for i in range(0, len(term_groups)):\n",
    "    term_group = term_groups[i]\n",
    "    for word in term_group:\n",
    "      if word in key_words:\n",
    "        min_window = max(0, i - context_window_length)\n",
    "        max_window = min(len(term_groups), i + context_window_length + 1)\n",
    "        for j in range(min_window, max_window):\n",
    "          group = term_groups[j]\n",
    "          for w in group:\n",
    "            term_to_term[word][w] += 1\n",
    "  return term_to_term\n",
    "\n",
    "# have a dictionary, where for each word we have its number of coappearences with other words\n",
    "def generate_term_to_term_matrix_single_doc_for_words(document, context_window_length, key_words): \n",
    "  term_to_term = defaultdict(lambda: defaultdict(int))\n",
    "  for i in range(0, len(document)):\n",
    "    if document[i] in key_words:\n",
    "      key_word = document[i]\n",
    "      min_window = max(0, i - context_window_length)\n",
    "      max_window = min(len(document), i + context_window_length + 1)\n",
    "      for j in range(min_window, max_window):\n",
    "        if i != j:\n",
    "          term_to_term[key_word][document[j]] += 1\n",
    "  return term_to_term\n",
    "\n",
    "def merge_term_to_term_dicts(term_to_term_dicts):\n",
    "  term_to_term = defaultdict(lambda: defaultdict(int))\n",
    "  for term_to_term_dict in term_to_term_dicts:\n",
    "    for (key_word, freqs) in term_to_term_dict.items():\n",
    "      for (value_word, freq) in freqs.items():\n",
    "        term_to_term[key_word][value_word] += freq\n",
    "  return term_to_term\n",
    "\n",
    "def get_key_words_count(term_to_term_dict):\n",
    "  counts = defaultdict(int)\n",
    "  for (key, freqs) in term_to_term_dict.items():\n",
    "    for (context, freq) in freqs.items():\n",
    "      counts[key] += freq\n",
    "  return counts\n",
    "\n",
    "def get_context_words_count(term_to_term_dict):\n",
    "  counts = defaultdict(int)\n",
    "  for (key, freqs) in term_to_term_dict.items():\n",
    "    for (context, freq) in freqs.items():\n",
    "      counts[context] += freq\n",
    "  return counts\n",
    "\n",
    "def get_coocurrences_alpha(context_counts, alpha=0.75):\n",
    "  sum = 0\n",
    "  for context_count in context_counts.values():\n",
    "    sum += pow(context_count, alpha)\n",
    "  return sum\n",
    "\n",
    "def generate_context_word_mapping(term_to_term_dict):\n",
    "  i = 0\n",
    "  mapping = {}\n",
    "  for (key, freqs) in term_to_term_dict.items():\n",
    "    for (context, freq) in freqs.items():\n",
    "      if context not in mapping:\n",
    "        mapping[context] = i\n",
    "        i += 1\n",
    "  return mapping\n",
    "\n",
    "def calculate_all_coocurrences(count_dict):\n",
    "  sum = 0\n",
    "  for val in count_dict.values():\n",
    "    sum += val\n",
    "  return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pmi_matrix(word_counts, context_counts, term_to_term_dict, \n",
    "                         context_mapping, coocurrence_count):\n",
    "  pmi_matrix = []\n",
    "  row_to_word = []\n",
    "  for (word, word_count) in word_counts.items():\n",
    "    row_to_word.append(word)\n",
    "    p_word = word_count / coocurrence_count\n",
    "    row = [0] * len(context_counts)\n",
    "    for (context, index) in context_mapping.items():\n",
    "      if context in term_to_term_dict[word]:\n",
    "        p_context = context_counts[context] / coocurrence_count\n",
    "        p_word_context = term_to_term_dict[word][context]\n",
    "        row[index] = np.log2(p_word_context/(p_context * p_word))\n",
    "    pmi_matrix.append(row)\n",
    "  return (row_to_word, pmi_matrix)\n",
    "\n",
    "def calculate_ppmi_matrix(word_counts, context_counts, term_to_term_dict, \n",
    "                         context_mapping, coocurrence_count):\n",
    "  (row_to_word, pmi_matrix) = calculate_pmi_matrix(word_counts, context_counts, term_to_term_dict, \n",
    "                         context_mapping, coocurrence_count)\n",
    "  ppmi_matrix = [[0 if x < 0 else x for x in row] for row in pmi_matrix]\n",
    "\n",
    "  return (row_to_word, ppmi_matrix)\n",
    "\n",
    "def calculate_ppmi_alpha_matrix(word_counts, context_counts, term_to_term_dict, \n",
    "                         context_mapping, coocurrence_count, context_count_alpha, alpha=0.75):\n",
    "  pmi_matrix = []\n",
    "  row_to_word = []\n",
    "  for (word, word_count) in word_counts.items():\n",
    "    row_to_word.append(word)\n",
    "    p_word = word_count / coocurrence_count\n",
    "    row = [0] * len(context_counts)\n",
    "    for (context, index) in context_mapping.items():\n",
    "      if context in term_to_term_dict[word]:\n",
    "        p_context = pow(context_counts[context],alpha) / context_count_alpha\n",
    "        p_word_context = term_to_term_dict[word][context]\n",
    "        row[index] = max(np.log2(p_word_context/(p_context * p_word)), 0)\n",
    "    pmi_matrix.append(row)\n",
    "  return (row_to_word, pmi_matrix)\n",
    "\n",
    "def clustering_get_accuracy(n_clusters, keys, matrix, cluster_method, flag_empty_clusters, print_cluster=False):   \n",
    "  if (cluster_method == \"kmeans\"):                \n",
    "    cluster_algo = KMeans(n_clusters)\n",
    "  elif (cluster_method == \"agglomerative\"):\n",
    "      cluster_algo = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters\n",
    "      )\n",
    "  elif (cluster_method == \"agglomerative_complete\"):\n",
    "    cluster_algo = AgglomerativeClustering(\n",
    "      n_clusters=n_clusters,\n",
    "      linkage=\"complete\",\n",
    "      affinity=\"cosine\"\n",
    "    )\n",
    "  cluster_algo.fit(matrix)\n",
    "  clusters = []\n",
    "  for i in range(50):\n",
    "    clusters.append(set())\n",
    "  i = 0\n",
    "  for label in cluster_algo.labels_:\n",
    "    clusters[label].add(keys[i])\n",
    "    i += 1\n",
    "  correct = 0\n",
    "  for cluster in clusters:\n",
    "    if (flag_empty_clusters and len(cluster) == 0):\n",
    "      print(\"EMPTY CLUSTER DETECTED\")\n",
    "    if (print_cluster): \n",
    "      print(cluster)\n",
    "    for word in cluster:\n",
    "      if word[::-1] in cluster:\n",
    "        correct += 1\n",
    "  return correct / len(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(generate_reverse_word_corpus, context_type, context_window, original_most_frequent_tokens, key_words, similiarity_measure, iterations, remove_stopwords, stem, cluster_method, verbose):\n",
    "  accuracy = np.zeros(iterations)\n",
    "  for i in range(iterations):\n",
    "    if (generate_reverse_word_corpus):\n",
    "      generate_corpus_half_tokens_reversed(original_corpus, original_most_frequent_tokens, True)\n",
    "    corpus = nltk.corpus.PlaintextCorpusReader(corpus_after_token_reversal, file_pattern)\n",
    "    # partition the corpus based on the context window type, a corpus can be partititoned into sentences, reviews or static word windows\n",
    "    if (context_type == \"sentence\" or context_type == \"reviews\"):\n",
    "      # document cleaning\n",
    "      contexts = [process_doc(corpus.raw(fileid), True, True, stem, remove_stopwords, True, context_type, key_words, True) for fileid in corpus.fileids()]\n",
    "      # contains term to term matricies for each context window\n",
    "      term_to_term_dicts = [generate_term_to_term_dict_single_doc(context, context_window, key_words) for context in contexts]\n",
    "    elif (context_type == \"words\"):\n",
    "      # document cleaning\n",
    "      contexts = [process_doc(corpus.raw(fileid), True, True, stem, remove_stopwords, True, context_type, key_words, True) for fileid in corpus.fileids()]\n",
    "      # contains term to term matricies for each context window\n",
    "      term_to_term_dicts = [generate_term_to_term_matrix_single_doc_for_words(context, context_window, key_words) for context in contexts]\n",
    "    \n",
    "    # merge the term to term matricies into one\n",
    "    term_to_term_dict = merge_term_to_term_dicts(term_to_term_dicts)\n",
    "\n",
    "    # used for the denominator probabilities in pmi and its derivables\n",
    "    word_counts = get_key_words_count(term_to_term_dict)\n",
    "    context_counts = get_context_words_count(term_to_term_dict)\n",
    "\n",
    "    # until now we have worked with dictionaries for time and space efficiency,\n",
    "    # for the following operations we will have to work with matricies\n",
    "\n",
    "    # utility function - constructs a mapping for each key to an integer\n",
    "    context_mapping = generate_context_word_mapping(term_to_term_dict)\n",
    "\n",
    "    # used in the nominator in pmi\n",
    "    # coocurrence_count = the number of overall coocorrence of any two tokens with the \n",
    "    # currently selected context window\n",
    "    coocurrence_count = calculate_all_coocurrences(word_counts)\n",
    "\n",
    "    # pmi, ppmi, ppmi with smoothing matricies construction\n",
    "    if (similiarity_measure == \"smooth_ppmi\"):\n",
    "      coocurrence_alpha = get_coocurrences_alpha(context_counts, 0.80)\n",
    "      (keys, encoding) = calculate_ppmi_alpha_matrix(word_counts, context_counts, term_to_term_dict, context_mapping, coocurrence_count, coocurrence_alpha, 0.80)\n",
    "    if (similiarity_measure == \"ppmi\"):\n",
    "      (keys, encoding) = calculate_ppmi_matrix(word_counts, context_counts, term_to_term_dict, context_mapping, coocurrence_count)\n",
    "    if (similiarity_measure == \"pmi\"):\n",
    "      (keys, encoding) = calculate_pmi_matrix(word_counts, context_counts, term_to_term_dict, context_mapping, coocurrence_count)\n",
    "    accuracy[i] = clustering_get_accuracy(50, keys, encoding, cluster_method, True, verbose)\n",
    "    if (verbose):\n",
    "      print(accuracy)\n",
    "      \n",
    "  return (\"Average accuracy:\", np.mean(accuracy), \"Standard deviation:\", np.std(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different context length experiments\n",
      "1 review: ('Average accuracy:', 0.4870000000000001, 'Standard deviation:', 0.0330302891298275)\n",
      "Sentences:\n",
      "1 Sentence: ('Average accuracy:', 0.45899999999999996, 'Standard deviation:', 0.03713488925525428)\n",
      "3 Sentences: ('Average accuracy:', 0.645, 'Standard deviation:', 0.04043513323831147)\n",
      "5 Sentences: ('Average accuracy:', 0.6769999999999998, 'Standard deviation:', 0.03702701716314723)\n",
      "7 Sentences: ('Average accuracy:', 0.692, 'Standard deviation:', 0.03969886648255841)\n",
      "2 words: ('Average accuracy:', 0.8591836734693876, 'Standard deviation:', 0.03528901319549098)\n",
      "4 words: ('Average accuracy:', 0.7744897959183674, 'Standard deviation:', 0.030595233368499907)\n",
      "6 words: ('Average accuracy:', 0.7071428571428571, 'Standard deviation:', 0.034316676981224974)\n",
      "10 words: ('Average accuracy:', 0.6744897959183673, 'Standard deviation:', 0.039507062617563135)\n",
      "12 words: ('Average accuracy:', 0.6377551020408163, 'Standard deviation:', 0.029486088319999763)\n"
     ]
    }
   ],
   "source": [
    "print(\"Different context length experiments\")\n",
    "\n",
    "print(\"1 review:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"reviews\", \n",
    "              context_window = 0, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Sentences:\")\n",
    "# The sentence with the word\n",
    "print(\"1 Sentence:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 0, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "# 1 sentence before the word, the sentence with the word and 1 sentence after the word\n",
    "print(\"3 Sentences:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "# 2 sentences before the word, the sentence with the word and 2 sentences after the word\n",
    "print(\"5 Sentences:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"7 Sentences:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 3, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"2 words:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "print(\"4 words:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "print(\"6 words:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 3, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "print(\"10 words:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 5, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "print(\"12 words:\", run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 6, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords experiments:\n",
      "Performance when REMOVING stopwords with a large context window: ('Average accuracy:', 0.68, 'Standard deviation:', 0.03162277660168378)\n",
      "Performance when KEEPING stopwords with a large context window: ('Average accuracy:', 0.7010000000000001, 'Standard deviation:', 0.02998332870112988)\n",
      "Performance when REMOVING stopwords with a small context window: ('Average accuracy:', 0.7724489795918366, 'Standard deviation:', 0.03666376372086059)\n",
      "Performance when KEEPING stopwords with a small context window: ('Average accuracy:', 0.8551020408163266, 'Standard deviation:', 0.0392028014536705)\n"
     ]
    }
   ],
   "source": [
    "print(\"Stopwords experiments:\")\n",
    "\n",
    "print(\"Performance when REMOVING stopwords with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=True, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance when KEEPING stopwords with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance when REMOVING stopwords with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=True, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance when KEEPING stopwords with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens,\n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming experiments:\n",
      "Performance when stemming with a large context window: ('Average accuracy:', 0.6359999999999999, 'Standard deviation:', 0.030724582991474434)\n",
      "Performance when NOT stemming with a large context window: ('Average accuracy:', 0.669, 'Standard deviation:', 0.032542280190545954)\n",
      "Performance when stemming with a small context window: ('Average accuracy:', 0.8418367346938774, 'Standard deviation:', 0.026510981748503214)\n",
      "Performance when NOT stemming with a small context window: ('Average accuracy:', 0.8642857142857142, 'Standard deviation:', 0.02261073449608197)\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming experiments:\")\n",
    "\n",
    "print(\"Performance when stemming with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=True, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance when NOT stemming with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance when stemming with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=True, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance when NOT stemming with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens,\n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI vs PPMI vs PPMI with smoothing experiments\n",
      "Performance with pmi stemming with a large context window: ('Average accuracy:', 0.6700000000000002, 'Standard deviation:', 0.040249223594996206)\n",
      "Performance with ppmi with a large context window: ('Average accuracy:', 0.701, 'Standard deviation:', 0.03064310689208911)\n",
      "Performance with ppmi with smoothing with a large context window: ('Average accuracy:', 0.6779999999999998, 'Standard deviation:', 0.03280243893371344)\n",
      "Performance with pmi with a small context window: ('Average accuracy:', 0.8561224489795919, 'Standard deviation:', 0.03059523336849989)\n",
      "Performance with ppmi with a small context window: ('Average accuracy:', 0.8642857142857142, 'Standard deviation:', 0.03244512876649928)\n",
      "Performance with ppmi with smoothing with a small context window: ('Average accuracy:', 0.8561224489795919, 'Standard deviation:', 0.027739341263404448)\n"
     ]
    }
   ],
   "source": [
    "print(\"PMI vs PPMI vs PPMI with smoothing experiments\")\n",
    "\n",
    "print(\"Performance with pmi stemming with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"pmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance with ppmi with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance with ppmi with smoothing with a large context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"sentence\", \n",
    "              context_window = 2, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative\", verbose=False\n",
    "              ))\n",
    "\n",
    "print(\"Performance with pmi with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"pmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "            ))\n",
    "\n",
    "\n",
    "print(\"Performance with ppmi with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "\n",
    "\n",
    "print(\"Performance with ppmi with smoothing with a small context window:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 20,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=False\n",
    "              ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tnaw', 'want'}\n",
      "{'tcudorp', 'ipod'}\n",
      "{'computer', 'retupmoc'}\n",
      "{'tsrif', 'first'}\n",
      "{'esu', 'get', 'use', 'teg'}\n",
      "{'much', 'hcum'}\n",
      "{'works', 'skrow', 'work', 'krow'}\n",
      "{'life', 'efil'}\n",
      "{'neve', 'even'}\n",
      "{'sound', 'dnuos'}\n",
      "{'osla', 'also'}\n",
      "{'evitaerc', 'creative'}\n",
      "{'easy', 'ysae'}\n",
      "{'llew', 'well'}\n",
      "{'dluow', 'would'}\n",
      "{'pictures', 'serutcip'}\n",
      "{'erawtfos', 'software'}\n",
      "{'diaper', 'repaid'}\n",
      "{'ecirp', 'price'}\n",
      "{'buy', 'yub'}\n",
      "{'problem', 'melborp'}\n",
      "{'size', 'ezis'}\n",
      "{'dopi', 'reyalp', 'player'}\n",
      "{'aremac', 'camera'}\n",
      "{'notron', 'norton'}\n",
      "{'really', 'yllaer'}\n",
      "{'orcim'}\n",
      "{'time', 'emit'}\n",
      "{'ekil', 'like'}\n",
      "{'yrettab', 'battery'}\n",
      "{'deen', 'need'}\n",
      "{'phone', 'enohp'}\n",
      "{'cisum', 'music'}\n",
      "{'good', 'doog', 'taerg', 'great'}\n",
      "{'desu', 'used'}\n",
      "{'zen', 'nez'}\n",
      "{'llits'}\n",
      "{'little', 'elttil'}\n",
      "{'features', 'serutaef'}\n",
      "{'ytilauq', 'quality'}\n",
      "{'gnisu'}\n",
      "{'product'}\n",
      "{'router', 'retuor'}\n",
      "{'using'}\n",
      "{'still'}\n",
      "{'better', 'retteb'}\n",
      "{'eno', 'one'}\n",
      "{'pmahc'}\n",
      "{'champ'}\n",
      "{'micro'}\n",
      "[0.87755102]\n",
      "Clusters with best performing method: ('Average accuracy:', 0.8775510204081632, 'Standard deviation:', 0.0)\n"
     ]
    }
   ],
   "source": [
    "# running for a single iteration so as to show how usually the clusters with this method look like\n",
    "print(\"Clusters with best performing method:\",\n",
    "        run_experiment(generate_reverse_word_corpus = True, context_type = \"words\", \n",
    "              context_window = 1, original_most_frequent_tokens = most_frequent_tokens, \n",
    "              key_words = cluster_words, similiarity_measure=\"smooth_ppmi\", iterations = 1,\n",
    "              remove_stopwords=False, stem=False, cluster_method=\"agglomerative_complete\", verbose=True\n",
    "              ))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c921179475be10ab3872e10c2f4280fa2b556ee0d581886f6fc4b5fe61deff4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
