{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "7c921179475be10ab3872e10c2f4280fa2b556ee0d581886f6fc4b5fe61deff4"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Task1_Approach2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2qE1c-XzdyD",
        "outputId": "66299ab1-a8bb-424f-c840-38cec4b639ef"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import sample\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "nltk.download('stopwords')\n",
        "print(torch.cuda.device_count())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNui7uQP0FXc",
        "outputId": "7cf69fee-61b9-470a-9481-c7d876ff7c52"
      },
      "source": [
        "# I used google collab for training my model, uncomment the lines below to \n",
        "# connect to google drive in google colab\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9M8MrkbzdyE",
        "outputId": "e8eedd03-8106-485e-f68d-5b4fd0e064b2"
      },
      "source": [
        "\n",
        "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "stemmer = nltk.SnowballStemmer(\"english\", ignore_stopwords = False)\n",
        "# NB corpus_root AND corpus_after_token_reversal SHOULD BE CHANGED TO MATCH\n",
        "# THE CORPUS PATH ON THE SPECIFIC MACHINE\n",
        "# Folder path where corpus root should be\n",
        "corpus_root = r\"/content/drive/MyDrive/cw2/product_reviews\"\n",
        "# Folder path where the reverse token corpus should be stored\n",
        "corpus_after_token_reversal = r\"/content/drive/MyDrive/cw2/product_reviews_processed\"\n",
        "file_pattern = r\".*\"\n",
        "original_corpus = nltk.corpus.PlaintextCorpusReader(corpus_root, file_pattern)\n",
        "print(original_corpus.fileids())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Canon_PowerShot_SD500.txt', 'Canon_S100.txt', 'Diaper_Champ.txt', 'Hitachi_router.txt', 'Linksys_Router.txt', 'MicroMP3.txt', 'Nokia_6600.txt', 'ipod.txt', 'norton.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DsjTya9zdyF"
      },
      "source": [
        "# Core utility function for document cleaning\n",
        "# Works recursively, split the text into sentences/review, then for each \n",
        "# sentence/review perform cleaning \n",
        "def process_doc(text, remove_punctuation, case_fold, stem,\n",
        "                remove_stopwords, remove_short_tokens, tokenize_by, stem_blacklist = [],\n",
        "                remove_nonalphabetical = False):\n",
        "\n",
        "  if (tokenize_by == \"sentence\"):\n",
        "    sentences = nltk.RegexpTokenizer(\"##\", gaps = True).tokenize(text)\n",
        "    sentences = [process_doc(sentence, remove_punctuation, case_fold, stem, \n",
        "                             remove_stopwords, remove_short_tokens, \"words\", stem_blacklist) \n",
        "                  for sentence in sentences]\n",
        "    return sentences\n",
        "  if (tokenize_by == \"sentiments\"):\n",
        "    sentiments = nltk.RegexpTokenizer(\"\\[[\\-+] [0-9] \\]\", gaps = True).tokenize(text)\n",
        "    sentiments = [process_doc(sentiment, remove_punctuation, case_fold, stem, \n",
        "                              remove_stopwords, remove_short_tokens, \"words\", stem_blacklist)\n",
        "                  for sentiment in sentiments]\n",
        "    return sentiments\n",
        "  if (tokenize_by == \"reviews\"):\n",
        "    reviews = nltk.RegexpTokenizer(\"\\[ t \\]\", gaps = True).tokenize(text)\n",
        "    reviews = [process_doc(review, remove_punctuation, case_fold, stem, \n",
        "                              remove_stopwords, remove_short_tokens, \"words\", stem_blacklist)\n",
        "                for review in reviews]\n",
        "    return reviews\n",
        "  if (tokenize_by == \"words\"):\n",
        "    words = nltk.WordPunctTokenizer().tokenize(text)\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "      # words = [w.strip(\"\") for w in words]\n",
        "    if (case_fold):\n",
        "      words = [w.lower() for w in words]\n",
        "    if (remove_short_tokens):\n",
        "      words = [w for w in words if len(w) > 2]\n",
        "    if (stem):\n",
        "      words = [w if w in stem_blacklist else stemmer.stem(w) for w in words]\n",
        "    if (remove_stopwords):\n",
        "      words = [w for w in words if w not in stop_words and w != \"n't\"]\n",
        "    if (remove_punctuation):\n",
        "      words = [w for w in words if w not in string.punctuation and w != \"...\" and w != \"]##\"]\n",
        "    if (remove_nonalphabetical):\n",
        "      words = [w for w in words if w.isalpha()]\n",
        "    return words\n",
        "\n",
        "def process_corpus(corpus, remove_punctuation:bool, case_fold:bool, stem:bool,\n",
        "                  remove_stopwords:bool, remove_short_tokens, tokenize_by:str, remove_nonalphabetical):\n",
        "  docs = [word for fileid in corpus.fileids() \n",
        "            for word in process_doc(corpus.raw(fileid), remove_punctuation, case_fold,\n",
        "                                    stem, remove_stopwords, remove_short_tokens, \n",
        "                                    tokenize_by, remove_nonalphabetical)\n",
        "         ]\n",
        "  return docs\n",
        "\n",
        "def most_frequent(words, n, should_print):\n",
        "  freqDist = nltk.FreqDist(words)\n",
        "  most_common = freqDist.most_common(n)\n",
        "  if (should_print):\n",
        "    i = 1\n",
        "    for (w, count) in most_common:\n",
        "      print(i , w , count)\n",
        "      i += 1\n",
        "  return most_common\n",
        "\n",
        "# core function for generating corpus with reversed words\n",
        "# the corpus of reversed words is stored as files in the path specified by the variable:\n",
        "# corpus_after_token_reversal\n",
        "def generate_corpus_half_tokens_reversed(corpus, token_tuple_list, override_folder):\n",
        "  if not override_folder and os.path.exists(corpus_after_token_reversal):\n",
        "    return\n",
        "  if not os.path.exists(corpus_after_token_reversal):\n",
        "    os.mkdir(corpus_after_token_reversal)\n",
        "  # indecies_per_word = {word : list of 0s and 1s}\n",
        "  # if indecies_per_word[\"word\"][i] == 1\n",
        "  #   the i-th occurrence of \"word\" needs to be reversed\n",
        "  indecies_per_word = {}\n",
        "  # pointers keeps track of how many occurrences of each word we have met\n",
        "  pointers = {}\n",
        "  for (word, frequency) in token_tuple_list:\n",
        "    # construct an array with an equal number of 0-s and ones\n",
        "    indecies = np.ones(frequency)\n",
        "    indecies[:int(frequency/2)] = 0\n",
        "\n",
        "    # shuffle it\n",
        "    np.random.shuffle(indecies)\n",
        "    indecies_per_word[word] = indecies\n",
        "    pointers[word] = -1\n",
        "  fileids = corpus.fileids()\n",
        "  for fileid in fileids:\n",
        "    # tokenize the document\n",
        "    tokens = process_doc(corpus.raw(fileid), False, True, False, False, False, \"words\", False)\n",
        "    with_reversal = []\n",
        "    for token in tokens:\n",
        "      if (token in indecies_per_word):\n",
        "        # update the number of occurrences of the token\n",
        "        pointers[token] += 1\n",
        "        # determine whether to reverse the token\n",
        "        if (indecies_per_word[token][pointers[token]] == 1):\n",
        "         token = token[::-1]\n",
        "      with_reversal.append(token)\n",
        "    doc = \" \".join(with_reversal)\n",
        "    \n",
        "    f = open(os.path.join(corpus_after_token_reversal,fileid), \"w\")\n",
        "    f.write(doc)\n",
        "    f.close()\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXvXGnSwzdyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fa99aaa-35d2-44d8-bb19-4892579f7718"
      },
      "source": [
        "print(\"Most frequent 50 tokens in corpus after document cleaning and lemmatisation\")\n",
        "processed_corpus = process_corpus(original_corpus, True, True, False, True, True, \"words\", True)\n",
        "most_frequent_tokens = most_frequent(processed_corpus, 50, True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent 50 tokens in corpus after document cleaning and lemmatisation\n",
            "1 use 353\n",
            "2 phone 320\n",
            "3 one 316\n",
            "4 ipod 314\n",
            "5 router 313\n",
            "6 camera 292\n",
            "7 player 269\n",
            "8 get 252\n",
            "9 battery 239\n",
            "10 like 195\n",
            "11 great 192\n",
            "12 quality 176\n",
            "13 good 176\n",
            "14 zen 174\n",
            "15 diaper 171\n",
            "16 product 166\n",
            "17 would 158\n",
            "18 also 156\n",
            "19 time 145\n",
            "20 software 145\n",
            "21 sound 144\n",
            "22 well 138\n",
            "23 really 136\n",
            "24 micro 136\n",
            "25 features 128\n",
            "26 computer 128\n",
            "27 easy 125\n",
            "28 even 123\n",
            "29 first 121\n",
            "30 used 120\n",
            "31 creative 118\n",
            "32 much 115\n",
            "33 better 114\n",
            "34 champ 113\n",
            "35 work 112\n",
            "36 want 107\n",
            "37 size 105\n",
            "38 music 105\n",
            "39 norton 104\n",
            "40 little 101\n",
            "41 need 100\n",
            "42 pictures 99\n",
            "43 works 99\n",
            "44 still 97\n",
            "45 buy 96\n",
            "46 problem 96\n",
            "47 mp3 96\n",
            "48 price 91\n",
            "49 life 91\n",
            "50 using 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoqkRnCmzdyG"
      },
      "source": [
        "def get_all_sentences_cleaned(corpus_filepath, stemming, stopwords_removal, stem_blacklist = []):\n",
        "  corpus = nltk.corpus.PlaintextCorpusReader(corpus_filepath, file_pattern)\n",
        "  out = []\n",
        "  for fileid in corpus.fileids():\n",
        "    sentences = process_doc(corpus.raw(fileid), True, True, stemming, stopwords_removal, True, \"sentence\", stem_blacklist, True)\n",
        "    out.extend(sentences)\n",
        "  return out\n",
        "\n",
        "\n",
        "def generate_word_to_indx_and_idx_to_word(corpus):\n",
        "  word_to_idx = {}\n",
        "  idx_to_word = {}\n",
        "  i = 0\n",
        "  for sentence in corpus:\n",
        "    for word in sentence:\n",
        "      if (word not in word_to_idx):\n",
        "        word_to_idx[word] = i\n",
        "        idx_to_word[i] = word\n",
        "        i += 1\n",
        "  return (word_to_idx, idx_to_word)\n",
        "\n",
        "def get_context_window_tuples(word_to_idx, sentences, window, key_words):\n",
        "  tuples = []\n",
        "  for sentence in sentences:\n",
        "    for i in range(window, len(sentence) - window):\n",
        "        context = []\n",
        "        middle_word = word_to_idx[sentence[i]]\n",
        "        for j in range (i - window, i + window + 1):\n",
        "          if i != j:\n",
        "            context.append(word_to_idx[sentence[j]])\n",
        "        tuples.append((context, word_to_idx[sentence[i]]))\n",
        "          \n",
        "        \n",
        "  return tuples\n",
        "\n",
        "\n",
        "def get_skipgrams(sentences, word_to_idx, window, neg_sample_count):\n",
        "  word = []\n",
        "  context = []\n",
        "  y = []\n",
        "  for sentence in sentences:\n",
        "    for i in range(len(sentence)):\n",
        "      cont = [word_to_idx[sentence[idx]] for idx in range(max(0, i - window), min(len(sentence), i + window + 1)) if idx != i]\n",
        "      blacklist = set(cont)\n",
        "      word.extend([word_to_idx[sentence[i]]] * (len(cont)))\n",
        "      context.extend(cont)\n",
        "  return(word, context)    \n",
        "\n",
        "def get_batches(words, contexts, batch_size):\n",
        "  shuffled_idxs = sample(range(0, len(words)), len(words))\n",
        "  batches = []\n",
        "\n",
        "  batch_word, batch_context = [], []\n",
        "  for i in range(len(words)):\n",
        "    idx = shuffled_idxs[i]\n",
        "    batch_word.append(words[idx])\n",
        "    batch_context.append(contexts[idx])\n",
        "    if (i + 1) % batch_size == 0 or i + 1 == len(words):\n",
        "      batches.append((\n",
        "        torch.from_numpy(np.array(batch_word)),\n",
        "        torch.from_numpy(np.array(batch_context))\n",
        "      ))\n",
        "      batch_word, batch_context = [], []\n",
        "  return batches\n",
        "\n",
        "  \n",
        "def get_x_tensors(x_y_tuples):\n",
        "  tensors = []\n",
        "  for tuple in x_y_tuples:\n",
        "    tensors.append(torch.tensor(tuple[0], dtype=torch.long))\n",
        "  return tensors\n",
        "def get_y_tensors(tuples, num_classes):\n",
        "  tensors = []\n",
        "  for tuple in tuples:\n",
        "    tensors.append(torch.tensor([tuple[1]]))\n",
        "  return tensors"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A61s4CmNzdyH"
      },
      "source": [
        "class Word2Vec_Skipgram(nn.Module):\n",
        "  def __init__(self, embedding_size, vocab_size) -> None:\n",
        "        super(Word2Vec_Skipgram, self).__init__()\n",
        "        self.embedding_words = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim = 1)\n",
        "    \n",
        "  def forward(self, words):\n",
        "      words_emb = self.embedding_words(words)\n",
        "      scores = self.linear(words_emb)\n",
        "      log_probs = self.log_softmax(scores)\n",
        "      return log_probs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKxnnor5azM_"
      },
      "source": [
        "def train_skipgram_model(model, epochs, batch_size, learning_rate, verbose, words, contexts):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  loss_function = nn.NLLLoss()\n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for inputs,targets in get_batches(words=words, contexts=contexts, batch_size=300):\n",
        "      optimizer.zero_grad()\n",
        "      inputs, targets = inputs.to(device), targets.to(device)\n",
        "      y_hat = model(inputs)\n",
        "      loss = loss_function(y_hat, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss\n",
        "    if (verbose):\n",
        "      print(epoch, total_loss)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yft2FzozdyI"
      },
      "source": [
        "def clustering_get_accuracy(n_clusters, keys, matrix, cluster_method, flag_empty_clusters, print_cluster=False):   \n",
        "  if (cluster_method == \"kmeans\"):                \n",
        "    cluster_algo = KMeans(n_clusters)\n",
        "  elif (cluster_method == \"agglomerative\"):\n",
        "      cluster_algo = AgglomerativeClustering(\n",
        "        n_clusters=n_clusters\n",
        "      )\n",
        "  elif (cluster_method == \"agglomerative_complete\"):\n",
        "    cluster_algo = AgglomerativeClustering(\n",
        "      n_clusters=n_clusters,\n",
        "      linkage=\"complete\",\n",
        "      affinity=\"cosine\"\n",
        "    )\n",
        "  cluster_algo.fit(matrix)\n",
        "  clusters = []\n",
        "  for i in range(50):\n",
        "    clusters.append(set())\n",
        "  i = 0\n",
        "  for label in cluster_algo.labels_:\n",
        "    clusters[label].add(keys[i])\n",
        "    i += 1\n",
        "  correct = 0\n",
        "  for cluster in clusters:\n",
        "    if (flag_empty_clusters and len(cluster) == 0):\n",
        "      print(\"EMPTY CLUSTER DETECTED\")\n",
        "    if (print_cluster): \n",
        "      print(cluster)\n",
        "    for word in cluster:\n",
        "      if word[::-1] in cluster:\n",
        "        correct += 1\n",
        "  return correct / len(keys)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_target_words_embeddings(target_words, embedding_matrix, word_to_idx):\n",
        "  keys = []\n",
        "  matrix = []\n",
        "  for key in target_words:\n",
        "    idx = word_to_idx[key]\n",
        "    matrix.append(embedding_matrix[idx])\n",
        "    keys.append(key)\n",
        "  return (keys, matrix)"
      ],
      "metadata": {
        "id": "149Lz609Fj1G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(iterations, corpus_root, training_epochs, embedding_dims, window_size, cluster_method, learning_rate, stemming, stopwords_removal, verbose = False):\n",
        "  accuracy = np.zeros(iterations)\n",
        "  # Get the 50 most common words for the experiment\n",
        "  processed_corpus = process_corpus(original_corpus, True, True, False, True, True, \"words\", True)\n",
        "  most_frequent_tokens = most_frequent(processed_corpus, 50, False)\n",
        "  cluster_words = set()\n",
        "  for (word, freq) in most_frequent_tokens:\n",
        "    cluster_words.add(word)\n",
        "    cluster_words.add(word[::-1])\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    # reverse half of instances of most common words at random\n",
        "    generate_corpus_half_tokens_reversed(original_corpus, most_frequent_tokens, True)\n",
        "    # clean sentences\n",
        "    sentences = get_all_sentences_cleaned(corpus_after_token_reversal, stemming, stopwords_removal, cluster_words)\n",
        "\n",
        "    # set up data\n",
        "    (word_to_idx, idx_to_word) = generate_word_to_indx_and_idx_to_word(sentences)\n",
        "    vocab_size = len(word_to_idx)\n",
        "    tuples = get_context_window_tuples(word_to_idx, sentences, window_size, cluster_words)\n",
        "    (words, contexts) = get_skipgrams(sentences, word_to_idx, window_size, 10)\n",
        "\n",
        "    # train model\n",
        "    skipgrams_model = Word2Vec_Skipgram(embedding_dims, vocab_size=vocab_size).to(device)\n",
        "    train_skipgram_model(skipgrams_model, training_epochs, 500, learning_rate, False, words, contexts)\n",
        "\n",
        "    # get embeddings\n",
        "    embedding_matrix = skipgrams_model.embedding_words.weight.detach().cpu().numpy()\n",
        "    (target_words, embeddings) = get_target_words_embeddings(cluster_words, embedding_matrix, word_to_idx)\n",
        "\n",
        "    # perform clustering\n",
        "    accuracy[iteration] = clustering_get_accuracy(50, target_words, embeddings, cluster_method, True, verbose)\n",
        "    if (verbose):\n",
        "      print(\"Iteration\", iteration + 1, \"Accuracy:\", accuracy[iteration])\n",
        "  return (\"Average accuracy:\", np.mean(accuracy), \"Standard diviation:\", np.std(accuracy))"
      ],
      "metadata": {
        "id": "mWdGb43RcTn3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance with window size 1:\",\n",
        "      run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=100, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with window size 2:\",\n",
        "      run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=100, window_size=2, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with window size 3:\",\n",
        "      run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=100, window_size=3, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False)) \n",
        "\n",
        "print(\"Performance with window size 5:\",\n",
        "      run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=100, window_size=5, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evm1QqV6lXnR",
        "outputId": "4460421a-a991-4285-caa6-724d1ab222de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance with window size 1: ('Average accuracy:', 0.876, 'Standard diviation:', 0.03200000000000003)\n",
            "Performance with window size 2: ('Average accuracy:', 0.8560000000000001, 'Standard diviation:', 0.008000000000000007)\n",
            "Performance with window size 3: ('Average accuracy:', 0.8400000000000001, 'Standard diviation:', 0.055136195008360894)\n",
            "Performance with window size 5: ('Average accuracy:', 0.716, 'Standard diviation:', 0.06499230723708765)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance with word embedding length 50:\",\n",
        "    run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=50, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with embedding length 100:\",\n",
        "    run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=100, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with embedding length 150:\",\n",
        "    run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=150, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with embedding length 200:\",\n",
        "    run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=200, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "print(\"Performance with embedding length 300:\",\n",
        "    run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with embedding length 400:\",\n",
        "    run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=400, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuQhzWZX_ZSE",
        "outputId": "7bc49cc4-919f-48ce-a364-d8a3aeb7a44f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance with word embedding length 50: ('Average accuracy:', 0.8280000000000001, 'Standard diviation:', 0.07440430095095311)\n",
            "Performance with embedding length 100: ('Average accuracy:', 0.884, 'Standard diviation:', 0.034409301068170535)\n",
            "Performance with embedding length 150: ('Average accuracy:', 0.884, 'Standard diviation:', 0.034409301068170535)\n",
            "Performance with embedding length 200: ('Average accuracy:', 0.9, 'Standard diviation:', 0.02529822128134702)\n",
            "Performance with embedding length 300: ('Average accuracy:', 0.9, 'Standard diviation:', 0.041952353926806046)\n",
            "Performance with embedding length 400: ('Average accuracy:', 0.8799999999999999, 'Standard diviation:', 0.03346640106136301)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(\"Stemming: \",\n",
        "       run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=True, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Performance with stopwords removal:\",\n",
        "      run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=True, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"Baseline: \",\n",
        "       run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiuRVwbGv9w-",
        "outputId": "96099356-a795-4854-eea0-c926f47d3511"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming:  ('Average accuracy:', 0.9079999999999998, 'Standard diviation:', 0.03249615361854383)\n",
            "Performance with stopwords removal: ('Average accuracy:', 0.7799999999999999, 'Standard diviation:', 0.04)\n",
            "Baseline:  ('Average accuracy:', 0.924, 'Standard diviation:', 0.03199999999999998)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"10 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=10, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"20 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"30 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=30, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))\n",
        "\n",
        "print(\"40 Epochs: \",\n",
        "       run_experiment(iterations=5, corpus_root=corpus_root, training_epochs=40, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ibmNqGKv9-8",
        "outputId": "d6d1e2b3-e252-4ca4-823f-2950f5adff6d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 Epochs:  ('Average accuracy:', 0.916, 'Standard diviation:', 0.019595917942265412)\n",
            "20 Epochs:  ('Average accuracy:', 0.9199999999999999, 'Standard diviation:', 0.012649110640673493)\n",
            "30 Epochs:  ('Average accuracy:', 0.8959999999999999, 'Standard diviation:', 0.03440930106817049)\n",
            "40 Epochs:  ('Average accuracy:', 0.876, 'Standard diviation:', 0.04270831300812524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " print(\"Best performance: \",\n",
        "       run_experiment(iterations = 5, corpus_root=corpus_root, training_epochs=20, embedding_dims=300, window_size=1, \n",
        "               cluster_method=\"agglomerative_complete\", learning_rate=0.01, stemming=False, stopwords_removal=False, \n",
        "               verbose=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpCJ73hD7A7c",
        "outputId": "d5de9b0e-870b-4bd5-f65e-77b10ab1928d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'serutaef', 'features'}\n",
            "{'teg', 'get'}\n",
            "{'need', 'want'}\n",
            "{'erawtfos', 'using'}\n",
            "{'doog', 'taerg', 'good', 'great'}\n",
            "{'orcim', 'creative', 'evitaerc', 'micro'}\n",
            "{'elttil', 'little'}\n",
            "{'osla', 'also'}\n",
            "{'desu', 'used'}\n",
            "{'tsrif', 'first'}\n",
            "{'product', 'tcudorp'}\n",
            "{'even', 'neve'}\n",
            "{'software', 'nez', 'zen'}\n",
            "{'computer', 'retupmoc'}\n",
            "{'enohp', 'phone'}\n",
            "{'dnuos', 'sound'}\n",
            "{'yllaer', 'really'}\n",
            "{'reyalp', 'player'}\n",
            "{'ipod', 'dopi'}\n",
            "{'3pm', 'mp3'}\n",
            "{'would', 'dluow'}\n",
            "{'deen', 'tnaw'}\n",
            "{'hcum', 'much'}\n",
            "{'price', 'ecirp'}\n",
            "{'llew', 'well'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'ezis', 'size'}\n",
            "{'one', 'eno'}\n",
            "{'retuor', 'router'}\n",
            "{'notron', 'norton'}\n",
            "{'life', 'efil'}\n",
            "{'use', 'esu'}\n",
            "{'time', 'emit'}\n",
            "{'like'}\n",
            "{'works', 'skrow'}\n",
            "{'yub', 'buy'}\n",
            "{'retteb', 'better'}\n",
            "{'champ', 'pmahc'}\n",
            "{'ysae', 'easy'}\n",
            "{'ytilauq', 'quality'}\n",
            "{'ekil'}\n",
            "{'cisum', 'music'}\n",
            "{'still'}\n",
            "{'llits'}\n",
            "{'battery', 'yrettab'}\n",
            "{'melborp', 'problem'}\n",
            "{'camera', 'aremac'}\n",
            "{'krow', 'work'}\n",
            "{'gnisu'}\n",
            "{'repaid', 'diaper'}\n",
            "Iteration 1 Accuracy: 0.88\n",
            "{'nez', 'reyalp', 'player', 'zen'}\n",
            "{'doog', 'taerg', 'good', 'great'}\n",
            "{'even', 'neve'}\n",
            "{'teg', 'get'}\n",
            "{'still', 'llits'}\n",
            "{'use', 'esu'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'ezis', 'size'}\n",
            "{'want', 'tnaw'}\n",
            "{'champ', 'pmahc'}\n",
            "{'osla', 'also'}\n",
            "{'tsrif', 'first'}\n",
            "{'llew', 'well'}\n",
            "{'computer', 'retupmoc'}\n",
            "{'retteb', 'better'}\n",
            "{'price', 'ecirp'}\n",
            "{'works', 'skrow'}\n",
            "{'krow', 'work'}\n",
            "{'time', 'emit'}\n",
            "{'erawtfos', 'software'}\n",
            "{'enohp', 'phone'}\n",
            "{'3pm', 'mp3'}\n",
            "{'need', 'deen'}\n",
            "{'retuor', 'router'}\n",
            "{'dnuos', 'sound'}\n",
            "{'life', 'efil'}\n",
            "{'notron', 'norton'}\n",
            "{'elttil', 'little'}\n",
            "{'orcim', 'creative', 'evitaerc', 'micro'}\n",
            "{'desu', 'used'}\n",
            "{'ekil', 'like'}\n",
            "{'product'}\n",
            "{'using'}\n",
            "{'melborp', 'problem'}\n",
            "{'hcum', 'much'}\n",
            "{'ytilauq', 'quality'}\n",
            "{'one', 'eno'}\n",
            "{'ipod', 'dopi'}\n",
            "{'yub', 'buy'}\n",
            "{'yllaer', 'really'}\n",
            "{'repaid', 'diaper'}\n",
            "{'battery', 'yrettab'}\n",
            "{'tcudorp'}\n",
            "{'ysae', 'easy'}\n",
            "{'gnisu'}\n",
            "{'cisum', 'music'}\n",
            "{'features'}\n",
            "{'camera', 'aremac'}\n",
            "{'would', 'dluow'}\n",
            "{'serutaef'}\n",
            "Iteration 2 Accuracy: 0.94\n",
            "{'orcim', 'creative', 'evitaerc', 'micro'}\n",
            "{'yub', 'buy'}\n",
            "{'yllaer', 'really', 'still'}\n",
            "{'osla', 'also'}\n",
            "{'teg', 'get'}\n",
            "{'ezis', 'size'}\n",
            "{'time', 'emit'}\n",
            "{'reyalp', 'player'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'llew', 'well'}\n",
            "{'doog', 'taerg', 'good', 'great'}\n",
            "{'want', 'tnaw'}\n",
            "{'erawtfos', 'software'}\n",
            "{'elttil', 'little'}\n",
            "{'one', 'eno'}\n",
            "{'even', 'neve'}\n",
            "{'krow', 'work'}\n",
            "{'would', 'dluow'}\n",
            "{'ipod', 'dopi'}\n",
            "{'desu', 'used'}\n",
            "{'melborp', 'problem'}\n",
            "{'ekil', 'like'}\n",
            "{'tsrif', 'first'}\n",
            "{'cisum', 'music'}\n",
            "{'use', 'esu'}\n",
            "{'serutaef', 'features'}\n",
            "{'life', 'efil'}\n",
            "{'product', 'tcudorp'}\n",
            "{'using'}\n",
            "{'hcum', 'much'}\n",
            "{'retuor', 'router'}\n",
            "{'dnuos', 'sound'}\n",
            "{'enohp', 'phone'}\n",
            "{'battery', 'yrettab'}\n",
            "{'champ', 'pmahc'}\n",
            "{'llits'}\n",
            "{'retteb', 'better'}\n",
            "{'3pm', 'mp3'}\n",
            "{'repaid', 'diaper'}\n",
            "{'ytilauq', 'quality'}\n",
            "{'retupmoc'}\n",
            "{'works', 'skrow'}\n",
            "{'notron', 'norton'}\n",
            "{'price', 'ecirp'}\n",
            "{'need', 'deen'}\n",
            "{'camera', 'aremac'}\n",
            "{'computer'}\n",
            "{'gnisu'}\n",
            "{'ysae', 'easy'}\n",
            "{'nez', 'zen'}\n",
            "Iteration 3 Accuracy: 0.94\n",
            "{'ezis', 'battery', 'size', 'yrettab'}\n",
            "{'computer', 'retupmoc'}\n",
            "{'creative', 'evitaerc', 'nez', 'zen'}\n",
            "{'doog', 'taerg', 'good', 'great'}\n",
            "{'ekil', 'like'}\n",
            "{'llew', 'well'}\n",
            "{'osla', 'also'}\n",
            "{'using', 'gnisu'}\n",
            "{'need', 'deen', 'tnaw'}\n",
            "{'hcum', 'much'}\n",
            "{'tsrif', 'first'}\n",
            "{'enohp', 'phone'}\n",
            "{'even', 'neve'}\n",
            "{'dnuos', 'sound'}\n",
            "{'reyalp', 'player'}\n",
            "{'time', 'emit'}\n",
            "{'still', 'llits'}\n",
            "{'yub', 'buy'}\n",
            "{'melborp', 'problem'}\n",
            "{'notron', 'norton'}\n",
            "{'teg', 'get'}\n",
            "{'would', 'dluow'}\n",
            "{'champ', 'pmahc'}\n",
            "{'elttil', 'little'}\n",
            "{'works', 'skrow'}\n",
            "{'camera', 'aremac'}\n",
            "{'ytilauq', 'quality'}\n",
            "{'yllaer', 'really'}\n",
            "{'erawtfos', 'software'}\n",
            "{'retteb', 'better'}\n",
            "{'krow', 'work'}\n",
            "{'use', 'esu'}\n",
            "{'retuor', 'router'}\n",
            "{'desu', 'used'}\n",
            "{'ipod', 'dopi'}\n",
            "{'pictures'}\n",
            "{'life', 'efil'}\n",
            "{'orcim', 'micro'}\n",
            "{'price', 'ecirp'}\n",
            "{'3pm', 'mp3'}\n",
            "{'product'}\n",
            "{'one', 'eno'}\n",
            "{'features'}\n",
            "{'repaid', 'diaper'}\n",
            "{'serutcip'}\n",
            "{'ysae', 'easy'}\n",
            "{'want'}\n",
            "{'tcudorp'}\n",
            "{'serutaef'}\n",
            "{'cisum', 'music'}\n",
            "Iteration 4 Accuracy: 0.92\n",
            "{'doog', 'taerg', 'good', 'great'}\n",
            "{'still', 'llits'}\n",
            "{'orcim', 'creative', 'evitaerc', 'micro'}\n",
            "{'osla', 'also'}\n",
            "{'ezis', 'size'}\n",
            "{'deen', 'need', 'want'}\n",
            "{'desu', 'used'}\n",
            "{'yllaer', 'really'}\n",
            "{'teg', 'get'}\n",
            "{'would', 'dluow'}\n",
            "{'works', 'krow', 'skrow'}\n",
            "{'ekil', 'like'}\n",
            "{'product', 'tcudorp'}\n",
            "{'using', 'gnisu'}\n",
            "{'elttil', 'little'}\n",
            "{'champ', 'pmahc'}\n",
            "{'dnuos', 'sound'}\n",
            "{'retteb', 'better'}\n",
            "{'one', 'eno'}\n",
            "{'notron', 'norton'}\n",
            "{'even', 'neve'}\n",
            "{'hcum', 'much'}\n",
            "{'tsrif', 'first'}\n",
            "{'llew', 'well'}\n",
            "{'ipod', 'dopi'}\n",
            "{'price', 'ecirp'}\n",
            "{'computer', 'retupmoc'}\n",
            "{'time', 'emit'}\n",
            "{'serutcip', 'pictures'}\n",
            "{'retuor', 'router'}\n",
            "{'enohp', 'phone'}\n",
            "{'use', 'esu'}\n",
            "{'life', 'efil'}\n",
            "{'melborp', 'problem'}\n",
            "{'erawtfos'}\n",
            "{'cisum', 'music'}\n",
            "{'ytilauq', 'quality'}\n",
            "{'battery', 'yrettab'}\n",
            "{'yub', 'buy'}\n",
            "{'features'}\n",
            "{'work'}\n",
            "{'3pm', 'mp3'}\n",
            "{'serutaef'}\n",
            "{'camera', 'aremac'}\n",
            "{'nez', 'zen'}\n",
            "{'reyalp', 'player'}\n",
            "{'ysae', 'easy'}\n",
            "{'software'}\n",
            "{'tnaw'}\n",
            "{'repaid', 'diaper'}\n",
            "Iteration 5 Accuracy: 0.92\n",
            "Best performance:  ('Average accuracy:', 0.9199999999999999, 'Standard diviation:', 0.021908902300206624)\n"
          ]
        }
      ]
    }
  ]
}